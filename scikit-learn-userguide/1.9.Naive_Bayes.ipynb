{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms\n",
    "based on applying Bayes’ theorem with the “naive” assumption of\n",
    "conditional independence between every pair of features given the\n",
    "value of the class variable. Bayes’ theorem states the following\n",
    "relationship, given class variable $y$ and dependent feature\n",
    "vector $x_1$ though $x_n$ :\n",
    "$$ P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)} $$\n",
    "                                 \n",
    "Using the naive conditional independence assumption that\n",
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),$$\n",
    "for all $i$ , this relationship is simplified to\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "Since $P(x_1, \\dots, x_n)$ is constant given the input, we can use the following classification rule:\n",
    "\n",
    "$$ \\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms\n",
    "based on applying Bayes’ theorem with the “naive” assumption of\n",
    "conditional independence between every pair of features given the\n",
    "value of the class variable. Bayes’ theorem states the following\n",
    "relationship, given class variable (target 변수) $y$ and dependent feature vector (input 변수) $x_1$ though $x_n$ :\n",
    "$$ P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)} $$\n",
    "  \n",
    "Using the naive conditional independence assumption that\n",
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),$$\n",
    "for all $i$ , this relationship is simplified to\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "**Likelihood $P(x_i \\mid y)$ 분포에 따라 Gaussian, Multinomial, ....**\n",
    "\n",
    "Since $P(x_1, \\dots, x_n)$ is constant given the input, we can use the following classification rule:\n",
    "\n",
    "$$ \\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:  \n",
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "The parameters $\\sigma_y$ and $ \\mu_y $ are estimated using maximum likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "# y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "gnbmodel = gnb.fit(X_train, y_train)\n",
    "y_pred = gnbmodel.predict(X_test)\n",
    "\n",
    "# fit/ partial_fit -> 대용량 데이터 한번에 계산하기에 메모리가 부족할 수 있을 때 사용\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(_BaseNB)\n",
      " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      " |  \n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like of shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, default=1e-9\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      class labels known to the classifier\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances\n",
      " |  \n",
      " |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  theta_ : ndarray of shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes,), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.38666666666666666, 1: 0.26666666666666666, 2: 0.3466666666666667}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train \n",
    "import numpy as np\n",
    "X_train, y_train\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "dict(zip(unique, counts/counts.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([29., 20., 26.]), {0: 29, 1: 20, 2: 26})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.class_count_, dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.38666667, 0.26666667, 0.34666667]),\n",
       " {0: 0.38666666666666666, 1: 0.26666666666666666, 2: 0.3466666666666667})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_prior 사전확률\n",
    "gnb.class_prior_, dict(zip(unique, counts/counts.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32126386, 0.36342931, 0.12763281, 0.0920115 ],\n",
       "       [0.50623611, 0.28792361, 0.50524747, 0.21447611],\n",
       "       [0.62345668, 0.31855815, 0.55949312, 0.21825018]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance of each feature per class (n_classes, n_features)\n",
    "gnb.sigma_\n",
    "\n",
    "# stdev of each feature per class (n_classes, n_features)\n",
    "gnb.sigma_**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.97586207, 3.35862069, 1.44827586, 0.23448276],\n",
       "       [5.935     , 2.71      , 4.185     , 1.3       ],\n",
       "       [6.77692308, 3.09230769, 5.73461538, 2.10769231]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of each feature per class (n_classes, n_features)\n",
    "gnb.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.7 3.1 4.7 1.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_test = X_test[7]\n",
    "print(one_test)\n",
    "\n",
    "gnbmodel.predict([one_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 확률이 가장 높은 label을 찾아가는 과정 확인\n",
    ">참고사이트 https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 8.516100787267677e-182, 2.0: 0.0015442142261661732, 1.0: 0.022042527983872816}\n"
     ]
    }
   ],
   "source": [
    "# 원본\n",
    "import pandas as pd\n",
    "from math import *\n",
    "exdf = pd.DataFrame(X_train)\n",
    "exdf['target'] = y_train\n",
    "exdf.head()\n",
    "\n",
    "exdataset = exdf.values\n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    " \n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries\n",
    " \n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries\n",
    " \n",
    "summaries = summarize_by_class(exdataset)\n",
    "summaries\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = np.exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    "\n",
    "x = one_test\n",
    "probabilities = calculate_class_probabilities(summaries, x)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.9146425371463955e-188, 1: 0.021007691878018386, 2: 0.0013468926536311186}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.classes_\n",
    "stdev = gnb.sigma_ **( 1/2 )\n",
    "mean = gnb.theta_\n",
    "x = one_test\n",
    "\n",
    "summaries = dict()\n",
    "for i in range(stdev.shape[0]):\n",
    "    summaries_class = []\n",
    "    for j in range(stdev.shape[1]):\n",
    "        summaries_class.append((mean[i,j], stdev[i,j], gnb.class_count_[i]))\n",
    "    summaries[gnb.classes_[i]] = summaries_class\n",
    "\n",
    "# 위에 summaries와 표준편차에서 차이가 조금 존재함\n",
    "summaries\n",
    "    \n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = np.exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "calculate_probability(x , mean, stdev)\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    "\n",
    "calculate_class_probabilities(summaries, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice).  \n",
    "\n",
    "분포는 각 클래스에 대해 벡터에 의해 매개 변수화된다.  \n",
    "The distribution is parametrized by vectors $ \\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn}) $ for each class $y$,  \n",
    "텍스트 분류에서 n은 단어의 수를 의미한다.  \n",
    "where $n$ is the number of features (in text classification, the size of the vocabulary) and  \n",
    "$ \\theta_{yi} $ is the probability $ P(x_i \\mid y) $ of feature  appearing in a sample belonging to class $y$.  \n",
    "  \n",
    "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
    "\n",
    "\n",
    "where $ N_{yi} = \\sum_{x \\in T} x_i $ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$,  \n",
    "and $N_{y} = \\sum_{i=1}^{n} N_{yi}$ is the total count of all features for class $y$.  \n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "mnb = MultinomialNB(alpha = 1)\n",
    "mnbmodel = mnb.fit(X_train, y_train)\n",
    "y_pred = mnbmodel.predict(X_test)\n",
    "\n",
    "# fit/ partial_fit -> 대용량 데이터 한번에 계산하기에 메모리가 부족할 수 있을 때 사용\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MultinomialNB in module sklearn.naive_bayes object:\n",
      "\n",
      "class MultinomialNB(_BaseDiscreteNB)\n",
      " |  MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |  \n",
      " |  Naive Bayes classifier for multinomial models\n",
      " |  \n",
      " |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      " |  discrete features (e.g., word counts for text classification). The\n",
      " |  multinomial distribution normally requires integer feature counts. However,\n",
      " |  in practice, fractional counts such as tf-idf may also work.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, default=1.0\n",
      " |      Additive (Laplace/Lidstone) smoothing parameter\n",
      " |      (0 for no smoothing).\n",
      " |  \n",
      " |  fit_prior : bool, default=True\n",
      " |      Whether to learn class prior probabilities or not.\n",
      " |      If false, a uniform prior will be used.\n",
      " |  \n",
      " |  class_prior : array-like of shape (n_classes,), default=None\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      Number of samples encountered for each class during fitting. This\n",
      " |      value is weighted by the sample weight when provided.\n",
      " |  \n",
      " |  class_log_prior_ : ndarray of shape (n_classes, )\n",
      " |      Smoothed empirical log probability for each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_classes, n_features)\n",
      " |      Mirrors ``feature_log_prob_`` for interpreting `MultinomialNB`\n",
      " |      as a linear model.\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n",
      " |          (renaming of 0.26).\n",
      " |  \n",
      " |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      " |      Number of samples encountered for each (class, feature)\n",
      " |      during fitting. This value is weighted by the sample weight when\n",
      " |      provided.\n",
      " |  \n",
      " |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      " |      Empirical log probability of features\n",
      " |      given a class, ``P(x_i|y)``.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (n_classes,)\n",
      " |      Mirrors ``class_log_prior_`` for interpreting `MultinomialNB`\n",
      " |      as a linear model.\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          ``intercept_`` is deprecated in 0.24 and will be removed in 1.1\n",
      " |          (renaming of 0.26).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      Number of features of each sample.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> rng = np.random.RandomState(1)\n",
      " |  >>> X = rng.randint(5, size=(6, 100))\n",
      " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      " |  >>> clf = MultinomialNB()\n",
      " |  >>> clf.fit(X, y)\n",
      " |  MultinomialNB()\n",
      " |  >>> print(clf.predict(X[2:3]))\n",
      " |  [3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      " |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      " |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      " |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      " |  https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultinomialNB\n",
      " |      _BaseDiscreteNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseDiscreteNB:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Naive Bayes classifier according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance overhead hence it is better to call\n",
      " |      partial_fit on chunks of data that are as large as possible\n",
      " |      (as long as fitting in the memory budget) to hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseDiscreteNB:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 참고사이트 https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is my book</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are novels</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have you read this book</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is the author</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are the characters</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is how I bought the book</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I like fictions</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is your favorite book</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sent     class\n",
       "0                This is my book      stmt\n",
       "1                They are novels      stmt\n",
       "2        have you read this book  question\n",
       "3              who is the author  question\n",
       "4        what are the characters  question\n",
       "5  This is how I bought the book      stmt\n",
       "6                I like fictions      stmt\n",
       "7     what is your favorite book  question"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['sent', 'class']\n",
    "rows = []\n",
    "\n",
    "rows = [['This is my book', 'stmt'], \n",
    "        ['They are novels', 'stmt'],\n",
    "        ['have you read this book', 'question'],\n",
    "        ['who is the author', 'question'],\n",
    "        ['what are the characters', 'question'],\n",
    "        ['This is how I bought the book', 'stmt'],\n",
    "        ['I like fictions', 'stmt'],\n",
    "        ['what is your favorite book', 'question']]\n",
    "\n",
    "training_data = pd.DataFrame(rows, columns=columns)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec_s = CountVectorizer()\n",
    "X_s = vec_s.fit_transform(training_data['sent'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>bought</th>\n",
       "      <th>characters</th>\n",
       "      <th>favorite</th>\n",
       "      <th>fictions</th>\n",
       "      <th>have</th>\n",
       "      <th>how</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>my</th>\n",
       "      <th>novels</th>\n",
       "      <th>read</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  author  book  bought  characters  favorite  fictions  have  how  is  \\\n",
       "0    0       0     1       0           0         0         0     0    0   1   \n",
       "1    1       0     0       0           0         0         0     0    0   0   \n",
       "2    0       0     1       0           0         0         0     1    0   0   \n",
       "3    0       1     0       0           0         0         0     0    0   1   \n",
       "4    1       0     0       0           1         0         0     0    0   0   \n",
       "5    0       0     1       1           0         0         0     0    1   1   \n",
       "6    0       0     0       0           0         0         1     0    0   0   \n",
       "7    0       0     1       0           0         1         0     0    0   1   \n",
       "\n",
       "   ...  my  novels  read  the  they  this  what  who  you  your  \n",
       "0  ...   1       0     0    0     0     1     0    0    0     0  \n",
       "1  ...   0       1     0    0     1     0     0    0    0     0  \n",
       "2  ...   0       0     1    0     0     1     0    0    1     0  \n",
       "3  ...   0       0     0    1     0     0     0    1    0     0  \n",
       "4  ...   0       0     0    1     0     0     1    0    0     0  \n",
       "5  ...   0       0     0    1     0     1     0    0    0     0  \n",
       "6  ...   0       0     0    0     0     0     0    0    0     0  \n",
       "7  ...   0       0     0    0     0     0     1    0    0     1  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stmt_docs = [row['sent'] for index,row in training_data.iterrows() if row['class'] == 'stmt']\n",
    "stmt_docs\n",
    "\n",
    "tdm = pd.DataFrame(X_s.toarray(), columns=vec_s.get_feature_names())\n",
    "tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tdm.values\n",
    "y = training_data['class']\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent     This is how I bought the book\n",
       "class                             stmt\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.iloc[5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stmt'], dtype='<U8')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict([X[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>bought</th>\n",
       "      <th>characters</th>\n",
       "      <th>favorite</th>\n",
       "      <th>fictions</th>\n",
       "      <th>have</th>\n",
       "      <th>how</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>my</th>\n",
       "      <th>novels</th>\n",
       "      <th>read</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  author  book  bought  characters  favorite  fictions  have  how  is  \\\n",
       "0    0       0     0       0           0         0         0     0    0   1   \n",
       "\n",
       "   ...  my  novels  read  the  they  this  what  who  you  your  \n",
       "0  ...   0       0     0    0     0     0     1    0    0     0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"what is that\"\n",
    "test_s = vec_s.transform([test])\n",
    "test_s.toarray()\n",
    "tdf_test2 = pd.DataFrame(test_s.toarray(), columns = vec_s.get_feature_names())\n",
    "tdf_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['question'], dtype='<U8')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(tdf_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ComplementNB implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the model’s weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks. The procedure for calculating the weights is as follows:  \n",
    "$$  \\begin{align}\\begin{aligned}\\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}\n",
    "                         {\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}\\\\w_{ci} = \\log \\hat{\\theta}_{ci}\\\\w_{ci} = \\frac{w_{ci}}{\\sum_{j} |w_{cj}|}\\end{aligned}\\end{align}  $$\n",
    "where the summations are over all documents $j$ not in class $c$,$d_{ij}$ is either the count or tf-idf value of term $i$ in document $j$, $\\alpha_i$ is a smoothing hyperparameter like that found in MNB, and $\\alpha = \\sum_{i} \\alpha_i$. The second normalization addresses the tendency for longer documents to dominate parameter estimates in MNB. The classification rule is:\n",
    "$$\\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}$$\n",
    "i.e., a document is assigned to the class that is the poorest complement match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tdm.values\n",
    "y = training_data['class']\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stmt'], dtype='<U8')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.predict([X[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).\n",
    "\n",
    "The decision rule for Bernoulli naive Bayes is based on\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n",
    "\n",
    "which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature $i$ that is an indicator for class $y$ , where the multinomial variant would simply ignore a non-occurring feature.\n",
    "\n",
    "In the case of text classification, **word occurrence vectors (rather than word count vectors)** may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is my book</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are novels</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have you read this book</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is the author</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are the characters</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is how I bought the book</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I like fictions</td>\n",
       "      <td>stmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is your favorite book</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sent     class\n",
       "0                This is my book      stmt\n",
       "1                They are novels      stmt\n",
       "2        have you read this book  question\n",
       "3              who is the author  question\n",
       "4        what are the characters  question\n",
       "5  This is how I bought the book      stmt\n",
       "6                I like fictions      stmt\n",
       "7     what is your favorite book  question"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>bought</th>\n",
       "      <th>characters</th>\n",
       "      <th>favorite</th>\n",
       "      <th>fictions</th>\n",
       "      <th>have</th>\n",
       "      <th>how</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>my</th>\n",
       "      <th>novels</th>\n",
       "      <th>read</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  author  book  bought  characters  favorite  fictions  have  how  is  \\\n",
       "0    0       0     1       0           0         0         0     0    0   1   \n",
       "1    1       0     0       0           0         0         0     0    0   0   \n",
       "2    0       0     1       0           0         0         0     1    0   0   \n",
       "3    0       1     0       0           0         0         0     0    0   1   \n",
       "4    1       0     0       0           1         0         0     0    0   0   \n",
       "5    0       0     1       1           0         0         0     0    1   1   \n",
       "6    0       0     0       0           0         0         1     0    0   0   \n",
       "7    0       0     1       0           0         1         0     0    0   1   \n",
       "\n",
       "   ...  my  novels  read  the  they  this  what  who  you  your  \n",
       "0  ...   1       0     0    0     0     1     0    0    0     0  \n",
       "1  ...   0       1     0    0     1     0     0    0    0     0  \n",
       "2  ...   0       0     1    0     0     1     0    0    1     0  \n",
       "3  ...   0       0     0    1     0     0     0    1    0     0  \n",
       "4  ...   0       0     0    1     0     0     1    0    0     0  \n",
       "5  ...   0       0     0    1     0     1     0    0    0     0  \n",
       "6  ...   0       0     0    0     0     0     0    0    0     0  \n",
       "7  ...   0       0     0    0     0     0     1    0    0     1  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word occurrence vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec_s = CountVectorizer()\n",
    "X_s = vec_s.fit_transform(training_data['sent'].values)\n",
    "\n",
    "wov = X_s.toarray()\n",
    "wov[wov>1] = 1\n",
    "wov\n",
    "\n",
    "tdf_test = pd.DataFrame(wov, columns = vec_s.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tdf_test.values\n",
    "y = training_data['class']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is that \n",
      " ['question']\n"
     ]
    }
   ],
   "source": [
    "print(test, \"\\n\", bnb.predict(tdf_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index $i$, has its own categorical distribution.\n",
    "\n",
    "For each feature $i$ in the training set $X$, CategoricalNB estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as $J = \\{ 1, \\dots, m \\}$, with $m$ as the number of samples.\n",
    "\n",
    "The probability of category $t$ in feature $i$ given class $c$ is estimated as:\n",
    "$$P(x_i = t \\mid y = c \\: ;\\, \\alpha) = \\frac{ N_{tic} + \\alpha}{N_{c} +\n",
    "                                       \\alpha n_i},$$\n",
    "where $N_{tic} = |\\{j \\in J \\mid x_{ij} = t, y_j = c\\}|$ is the number of times category $t$ appears in the samples $x_i$, which belong to class $c$,  \n",
    "$N_{c} = |\\{ j \\in J\\mid y_j = c\\}|$ is the number of samples with class c, $\\alpha$ is a smoothing parameter and $n_i$ is the number of available categories of feature $i$.  \n",
    "  \n",
    "CategoricalNB assumes that the sample matrix $X$ is encoded (for instance with the help of OrdinalEncoder) such that all categories for each feature $i$ are represented with numbers $0, ..., n_i - 1$ where $n_i$ is the number of available categories of feature $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Out-of-core naive Bayes model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting.\n",
    "\n",
    "Contrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels.\n",
    "\n",
    "For an overview of available strategies in scikit-learn, see also the out-of-core learning documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
